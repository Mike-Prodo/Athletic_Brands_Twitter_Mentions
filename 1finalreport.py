# -*- coding: utf-8 -*-
"""1FinalReport.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z9BBDjf3KecFVK6L42ny-kzwjbnBm06r

# Brand Twitter Mentions

For this project we will analyze Twitter data where athletic wear brands were mentioned, specifically Nike, Adidas, and Lululemon.

## The Data

We used the Twitter API, specifically the Search Endpoint, to retrieve Tweets that mention the three brands. We retrieved data for the last 93 days. These tweets are “at Mentions” (@nike, @lululemon, @adidas). These tweets were sent from the US and are in English. The data is in jsonl format.

## Imports
"""

import gzip
import json
import networkx as nx
import matplotlib.pyplot as plt
import re
import itertools
import nltk
import string
from textblob import TextBlob
from collections import Counter

"""## Data Preprocessing"""

from google.colab import drive
drive.mount('/content/drive')

"""Let's get our data and identify unique users in the mentions network."""

DATA_FILE = "drive/MyDrive/nikelululemonadidas_tweets.jsonl.gz"

users = {}

with gzip.open(DATA_FILE) as data_file:
    for i, line in enumerate(data_file):
        if i % 10000 == 0: # Show a periodic status
            print("%s tweets processed" % i)
        tweet = json.loads(line)
        user = tweet["user"]
        user_id = user["id"]
        if user_id not in users:
            users[user_id] = {
                "id": user_id,
                "tweet_count": 0,
                "followers_count": user["followers_count"]
            }
        users[user_id]["tweet_count"] += 1
    print(f"{i} total Tweets processed")

len(users)

"""As we can see, we have 175,077 tweets and 104,772 unique users. This is a lot of data! We will need to reduce this considerably in order to make meaningful graphs. We will only include tweets from users who have mentioned these brands at least twice and who have at least 100,000 followers."""

included_user_ids = []

min_tweet_count = 2
min_followers_count = 100000

for user_id, user in users.items():
    if user["tweet_count"] >= min_tweet_count and \
             user["followers_count"] >= min_followers_count:
        included_user_ids.append(user_id)

len(included_user_ids)

len(included_user_ids) / len(users)

"""With that criteria, we reduced our data from over 100,000 users to only 196, a 99% reduction. This will make our graph much better.

## Twitter Mentions Graph

Let's make our directed network graph of twitter mentions. This graph will illustrate who mentions who on Twitter, and in what way those mentions flow.
"""

graph = nx.DiGraph()

with gzip.open(DATA_FILE) as data_file:
    for i, line in enumerate(data_file):
        if i % 10000 == 0:
            print("%s tweets processed" % i)
        tweet = json.loads(line)
        sender_id = tweet["user"]["id"]
        sender_name = tweet["user"]["screen_name"]
        if sender_id in included_user_ids:
            for mention in tweet["entities"]["user_mentions"]:
                receiver_name = mention["screen_name"]
                receiver_id = mention["id"]
                if receiver_id in included_user_ids:
                    graph.add_edge(sender_name, receiver_name)

nx.number_of_nodes(graph)

nx.number_of_edges(graph)

"""Our graph has 126 nodes and 210 edges. Still a large graph, but with enough time we can make some meaningful conclusions about users who tweet about these three brands. The graph will be too big to be seen in this notebook, so we will save it to our drive to be able to properly view it."""

location = "/content/drive/MyDrive/MSDS_marketing_text_analytics/master_files/3_network_analysis/Network_Analysis"

fig, ax = plt.subplots(1, 1, figsize=(300, 300))
nx.draw_networkx(graph, ax=ax, font_color="#FFFFFF", font_size=20, node_size=30000, width=4, arrowsize=100)
plt.savefig("%s/mention_network.png" % location, format="PNG")

"""As mentioned, the graph cannot be viewed here but we were able to explore it separately. In the graph we can see one main cluster, which you'll find a separate image of this in the "Network_Analysis" folder. In this main cluster you can see the brand Adidas as the central node. You can see other brands close by such as Xbox and Burger King. There are also some sports teams associated with Adidas like Brooklyn Nets, LA Kings, LA Galaxy, and LA FC, and even a WNBA player Candace Parker. The NBA team Brooklyn Nets certainly acts as a bridge between brands as they have a sponsorship with Adidas, but the main NBA sponsor is Nike, which can be seen as contradicting. Jameson Lopp (@lopp) is a central individual twitter user. He is a co-founder of a digital finance/security company. It is interesting that he is a central user when it comes to athletic brands.

## Semantic Network Graph

Next, we'll create a semantic network analysis graph of words used in Tweets. Practically, this graph will reveal what words are most commonly associated with each other, for each brand. We will create one semantic graph, and that graph will have the data for all three brands.

We will need to download the following NLTK datasets.
"""

nltk.download("punkt")

nltk.download("stopwords")

nltk.download("wordnet")

"""In order to do this analysis we will need to perform tokenization. That is we will need to break down text into its component parts, most commonly in the form of words. As we are dealing with Twitter data, we will be using NLTK's specialized Tweet tokenizer. We will create a handful of functions to help clean our data as semantic network analysis of text is very messy.

The first function will remove hyperlinks from the data.
"""

def remove_links(tokens):
    """Removes http/s links from the tokens.

    This simple implementation assumes links have been kept intact as whole
    tokens. E.g. the way the Tweet Tokenizer works.
    """
    return [ t for t in tokens
            if not t.startswith("http://")
            and not t.startswith("https://")
        ]

"""The second function will remove stopwords from the text. Stopwords are common words that typically don't have value in analysis. NLTK has a corpus of stopwords built in."""

def remove_stopwords(tokens, stopwords=None):
    """Remove stopwords, i.e. words that we don't want as part of our
    analysis. Defaults to the default set of nltk english stopwords.
    """
    if stopwords is None:
        stopwords = nltk.corpus.stopwords.words("english")
    return [ token for token in tokens if token not in stopwords ]

"""The next function removes punctuation from the data."""

def remove_punctuation(tokens,
                       strip_mentions=False,
                       strip_hashtags=False,
                       strict=False):
    """Remove punctuation from a list of tokens.

    Has some specialized options for dealing with Tweets:

    strip_mentions=True will strip the @ off of @ mentions
    strip_hashtags=True will strip the # from hashtags

    strict=True will remove all punctuation from all tokens, not merely
    just tokens that are punctuation per se. 
    """
    tokens = [t for t in tokens if t not in string.punctuation]
    if strip_mentions:
        tokens = [t.lstrip('@') for t in tokens]
    if strip_hashtags:
        tokens = [t.lstrip('#') for t in tokens]
    if strict:
        cleaned = []
        for t in tokens:
            cleaned.append(
                t.translate(str.maketrans('', '', string.punctuation)).strip())
        tokens = [t for t in cleaned if t]
    return tokens

"""The final function will lemmatize the data. This is the process of normalizing words to their natural word form."""

LEMMATIZER = nltk.WordNetLemmatizer()

def lemmatize(tokens):
    """Lemmatize the tokens.
    
    Retains more natural word forms than stemming, but assumes all
    tokens are nouns unless tokens are passed as (word, pos) tuples.
    """
    lemmas = []
    for token in tokens:
        if isinstance(token, str):
            lemmas.append(LEMMATIZER.lemmatize(token)) # treats token like a noun
        else: # assume a tuple of (word, pos)
            lemmas.append(LEMMATIZER.lemmatize(*token))
    return lemmas

"""Now we are ready to implement our functions on the data and conduct our analysis."""

word_counts = {}
TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize

with gzip.open(DATA_FILE) as data_file:
    for i, line in enumerate(data_file):
        if i % 10000 == 0:
            print(f"Processed {i} tweets")
        tweet = json.loads(line)
        text = tweet["full_text"]
        text = text.lower()
        tokens = TWEET_TOKENIZER(text)
        tokens = remove_links(tokens)
        tokens = remove_stopwords(tokens)
        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)
        tokens = lemmatize(tokens) 
        for word in tokens:
            if word not in word_counts:
                word_counts[word] = 0
            word_counts[word] += 1

len(word_counts)

"""Even with cleaning our data, we still have 87,078 unique words. That is way too many words to perform semantic network analysis on so again we will need to greatly reduce our data. First, let's take a look at the 50 most common words."""

sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
sorted_words = [word for word, count in sorted_counts]
sorted_words[:50]

"""A couple things to note here is that we see some punctuation made it through our cleaning. We can also see that 'rt' is very common which makes sense for Twitter data as it siginifies a retweet, but doesn't make sense for our analysis. We will further clean our data by removing 'rt' and words shorter than 2 characters, which should also remove the extra punctuation. We'll do that by creating an additional function."""

def remove_words_shorter_than(tokens, length=2):
    """Removes words shorter than n characters long, 2 being default.
    """
    return [t for t in tokens if len(t) >= length]

word_counts = {}
TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize

stopwords = nltk.corpus.stopwords.words("english")
stopwords.append('rt')

with gzip.open(DATA_FILE) as data_file:
    for i, line in enumerate(data_file):
        if i % 10000 == 0:
            print(f"Processed {i} tweets")
        tweet = json.loads(line)
        text = tweet["full_text"]
        text = text.lower()
        tokens = TWEET_TOKENIZER(text)
        tokens = remove_links(tokens)
        tokens = remove_stopwords(tokens, stopwords=stopwords)
        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)
        tokens = lemmatize(tokens) 
        tokens = remove_words_shorter_than(tokens, length=2)
        for word in tokens:
            if word not in word_counts:
                word_counts[word] = 0
            word_counts[word] += 1

sorted_counts = sorted(word_counts.items(), key=lambda item: item[1], reverse=True)
sorted_words = [word for word, count in sorted_counts]
sorted_words[:50]

len(word_counts)

"""The list of the 50 most common words looks much better. We are still left with 86,046 unique words. To keep the size of our semantic network managable, we will reduce the word set to just the top 1000 most popular words.

We are now ready to build an undirected semantic network of co-occurring words that belong to our network of top n terms. These graphs can get kind of heavy, so for this project we'll create a small graph of n=20 to keep things manageable. More in-depth analysis would lead us to using larger graphs. We will need to add all of the 2-combinations (ie. co-occurrences) of included terms as an edge in the graph.
"""

N = 20
top_terms = sorted_words[:N]
graph = nx.Graph()
TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize

with gzip.open(DATA_FILE) as data_file:
    for i, line in enumerate(data_file):
        if i % 10000 == 0:
            print(f"Processed {i} tweets")
        tweet = json.loads(line)
        text = tweet["full_text"]
        text = text.lower()
        tokens = TWEET_TOKENIZER(text)
        tokens = remove_links(tokens)
        tokens = remove_stopwords(tokens, stopwords=stopwords)
        tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)
        tokens = lemmatize(tokens)
        tokens = remove_words_shorter_than(tokens, length=2)
        
        # reduce the tweet to terms in the 1000 word network and add the
        # term relationships to the graph
        nodes = [t for t in tokens if t in top_terms]
        cooccurrences = itertools.combinations(nodes, 2)
        graph.add_edges_from(cooccurrences)

nx.number_of_nodes(graph)

nx.number_of_edges(graph)

fig, ax = plt.subplots(1, 1, figsize=(300, 300))
nx.draw_networkx(graph, ax=ax, font_color="#FFFFFF", font_size=20, node_size=30000, width=4, arrowsize=100)
plt.savefig("%s/semantic_network.png" % location, format="PNG")

"""In this graph, only the brands Nike and Adidas are included. Lululemon was not tweeted about enough for this analysis. With more time, we would preprocess the data more so that Lululemon would also be analyzed.

In this graph, the words most closely related to Nike are ad, air, kingjames, jumpman23, and book. Most of these words make sense, as the word air is used a lot in their products, and kingjames (NBA player LeBron James) is sponsored by Nike and is one of the most known athletes. jumpman23 also makes sense as that is the twitter handle/username for Jordan, a sub-brand of Nike. Some further diving would be needed to see how book is related.
The words related to Adidas are not as clear. Some of these words are nikestore, win, try, xbox, and available. It is interesting to see that nikestore is related. One possible reason is that Adidas is often compared to Nike. Adidas and Xbox relationship makes sense as they have a partnership. The words that seem to be bridge words between Nike and Adidas are ad, day, win, try, and nikestore.

## Sentiment Analysis

To conduct our sentiment analysis, we will be using TextBlob, which calculates text sentiment, in this case Tweet sentiment. We will look at the sentiment of Twitter users with at least 100,000 followers who have mentioned these brands at least two times.
"""

negative_users = []
negative_tweets = []
positive_users = []
positive_tweets = []

with gzip.open(DATA_FILE) as data_file:
    for i, line in enumerate(data_file):
        if i % 10000 == 0:
            print("%s tweets processed" % i)
        tweet = json.loads(line)
        sender_id = tweet["user"]["id"]
        sender_name = tweet["user"]["screen_name"]
        if sender_id in included_user_ids:
          sentiment = TextBlob(tweet["full_text"])
          if sentiment.sentiment.polarity < 0.0:
            negative_users.append(sender_name)
            negative_tweets.append(tweet["full_text"])
          elif sentiment.sentiment.polarity > 0.0:
            positive_users.append(sender_name)
            positive_tweets.append(tweet["full_text"])

len(negative_users)

len(positive_users)

len(positive_users)/(len(positive_users)+len(negative_users))

"""We can see the vast majority (83%) of the tweets from these "big" Twitter users have been positive about the brands."""

negative_count = Counter(negative_users)
sorted_negative_count = negative_count.most_common()
filtered_negative_count = [(user, count) for user, count in sorted_negative_count if count >= 2]
users, counts = zip(*filtered_negative_count)

plt.figure(figsize=(10, 6))
plt.bar(users, counts)
plt.title('Negative Tweets from Users')
plt.xlabel('Twitter User')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.show()

"""We see that the user with the most negative tweets (6) was LEGIQN. Looking back at our mention network, this user was in the main cluster around Adidas. More in depth analysis of LEGIQN tweets might want to be looked at by Adidas to figure out why there are negative tweets. A couple more users that are interesting here are adidasHoops (3 negative tweets) and nikestore (2 negative tweets)."""

positive_count = Counter(positive_users)
sorted_positive_count = positive_count.most_common()
filtered_positive_count = [(user, count) for user, count in sorted_positive_count if count >= 4]
users, counts = zip(*filtered_positive_count)

plt.figure(figsize=(10, 6))
plt.bar(users, counts)
plt.title('Positive Tweets from Users')
plt.xlabel('Twitter User')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.show()

"""The user with the most positive tweets by far (42) is UNDEFEATEDinc, which was also in the main cluster of users around Adidas. nikestore is also included in this positive tweet list, but interestingly adidasHoops is not. You would think a Twitter account that is part of the brand family is more likely to be in the positive list than the negative list.

Finally, we'll look at word sentiments around each brand instead of users.
"""

negative_word_counts = {}
TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize

stopwords = nltk.corpus.stopwords.words("english")
stopwords.append('rt')

for tweet in negative_tweets:
      text = tweet.lower()
      tokens = TWEET_TOKENIZER(text)
      tokens = remove_links(tokens)
      tokens = remove_stopwords(tokens, stopwords=stopwords)
      tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)
      tokens = lemmatize(tokens) 
      tokens = remove_words_shorter_than(tokens, length=2)
      for word in tokens:
          if word not in negative_word_counts:
              negative_word_counts[word] = 0
          negative_word_counts[word] += 1

sorted_negative_counts = sorted(negative_word_counts.items(), key=lambda item: item[1], reverse=True)
words = [word[0] for word in sorted_negative_counts[:20]]
counts = [word[1] for word in sorted_negative_counts[:20]]

plt.figure(figsize=(10, 6))
plt.bar(words, counts)
plt.title('Words in Negative Tweets')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
for i, count in enumerate(counts):
    plt.text(i, count, str(count), ha='center', va='bottom')
plt.show()

"""We see Nike was mentioned the most (53 times) in negative tweets followed by Adidas (29 times), whereas lululemon was only mentioned 7 times (5th most word). There aren't really any clear negative words in this list, so more analysis would need to be done with more time. We also see that LeBron James, sponsored by Nike, is the athlete that gets the most negative tweets when these three brands are mentioned."""

positive_word_counts = {}
TWEET_TOKENIZER = nltk.TweetTokenizer().tokenize

stopwords = nltk.corpus.stopwords.words("english")
stopwords.append('rt')

for tweet in positive_tweets:
      text = tweet.lower()
      tokens = TWEET_TOKENIZER(text)
      tokens = remove_links(tokens)
      tokens = remove_stopwords(tokens, stopwords=stopwords)
      tokens = remove_punctuation(tokens, strip_mentions=True, strip_hashtags=True)
      tokens = lemmatize(tokens) 
      tokens = remove_words_shorter_than(tokens, length=2)
      for word in tokens:
          if word not in positive_word_counts:
              positive_word_counts[word] = 0
          positive_word_counts[word] += 1

sorted_positive_counts = sorted(positive_word_counts.items(), key=lambda item: item[1], reverse=True)
words = [word[0] for word in sorted_positive_counts[:20]]
counts = [word[1] for word in sorted_positive_counts[:20]]

plt.figure(figsize=(10, 6))
plt.bar(words, counts)
plt.title('Words in Positive Tweets')
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
for i, count in enumerate(counts):
    plt.text(i, count, str(count), ha='center', va='bottom')
plt.show()

"""Just like with negative tweets, Nike was also mentioned the most (297 times) in positive tweets followed by Adidas (145 times). Lululemon was the 4th most positive word mentioned (40 times). Even though Lululemon isn't mentioned the most out of the three brands, they have the best positive:negative ratio (5.7:1), followed by Nike (5.6:1), and then Adidas (5:1). Judging by that, Adidas is probably the brand that should analyze the negative tweets about themselves the most. Enes Kanter is the athlete with the most positive tweets when these three brands are mentioned. After some research, Enes Kanter spoke out against LeBron James and Nike for the company's alleged ties to forced labor in China, which suggests that more support is behind him in that controversy.

## Conclusion

This project was intended to create network graphs and conduct some sentimental analysis. From those results, we were able to make some simple, surface-level conclusions as stated throughout the project. From here, more research and analysis would be needed in order to draw more in depth conclusions. For example, as Adidas had the worse positive:negative ratio for tweets among the three brands, we would dive deeper into the graphs and sentiment analysis to possibly determine the reason behind that ratio. This a great start to what could be a very meaningful report on Twitter mentions for a few brands.
"""